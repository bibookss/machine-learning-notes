{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBLO1DlYHhnWQCc+8KeQv5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bibookss/machine-learning-notes/blob/main/computer-vision/HorsesOrHumans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download the training and validation data.\n",
        "\n",
        "Validation vs Test Data\n",
        "- Validation => during training\n",
        "- Test => after training"
      ],
      "metadata": {
        "id": "yaxh4pDZf0Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "training_url = \"https://storage.googleapis.com/learning-datasets/horse-or-human.zip\"\n",
        "validation_url = \"https://storage.googleapis.com/learning-datasets/validation-horse-or-human.zip\"\n",
        "\n",
        "file_name = \"horse-or-human.zip\"\n",
        "\n",
        "training_dir = 'horse-or-human/training/'\n",
        "validation_dir = 'horse-or-human/validation/'\n",
        "\n",
        "urllib.request.urlretrieve(training_url, file_name)\n",
        "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
        "zip_ref.extractall(training_dir)\n",
        "zip_ref.close()\n",
        "\n",
        "\n",
        "urllib.request.urlretrieve(validation_url, file_name)\n",
        "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
        "zip_ref.extractall(validation_dir)\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "NC82FZWMXDDb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create ImageDataGenerator\n",
        "ImageDataGenerator is used for preprocess such as image augmentation (changing stuff in images to create new data). It also assigns classes into files.\n"
      ],
      "metadata": {
        "id": "69lCJquegGkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_gen = ImageDataGenerator(\n",
        "    rescale=1/255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_data_gen.flow_from_directory(\n",
        "    training_dir,\n",
        "    target_size=(300, 300),\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "valid_data_gen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "valid_generator = valid_data_gen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(300,300),\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4g26k9wZ1SP",
        "outputId": "36942619-a84a-4114-c160-a408c282238c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create the model\n",
        "The model has many conv and pooling layers which aims to lower the size of the images. After all the conv and pooling layers, the size of the images are now 7 x 7. The model only needs to perform calcuations within the 49 pixels. But even with the small size, it needs to calculate many parameters still.\n",
        "\n",
        "The last output layer utilizes a sigmoid function which is used for binary classification."
      ],
      "metadata": {
        "id": "ZmB9Yvxphcma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300,300,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "oUHJltrtagal"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "WyMwuqykb9nt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMN5rZffcx2Y",
        "outputId": "80c7bd6a-b45f-450a-ee5e-7f9388899b20"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_10 (Conv2D)          (None, 298, 298, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPooli  (None, 149, 149, 16)      0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 147, 147, 32)      4640      \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPooli  (None, 73, 73, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 71, 71, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPooli  (None, 35, 35, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 33, 33, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPooli  (None, 16, 16, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 14, 14, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPooli  (None, 7, 7, 64)          0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 3136)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               1606144   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1704097 (6.50 MB)\n",
            "Trainable params: 1704097 (6.50 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=15,\n",
        "    validation_data=valid_generator\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8AZ5wUHcP3T",
        "outputId": "6ded61a8-61c8-4284-9c4d-938f80eeeb93"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "33/33 [==============================] - 32s 938ms/step - loss: 0.7049 - accuracy: 0.5229 - val_loss: 0.6620 - val_accuracy: 0.6602\n",
            "Epoch 2/15\n",
            "33/33 [==============================] - 36s 1s/step - loss: 0.6402 - accuracy: 0.6426 - val_loss: 0.5906 - val_accuracy: 0.6445\n",
            "Epoch 3/15\n",
            "33/33 [==============================] - 35s 1s/step - loss: 0.5606 - accuracy: 0.7235 - val_loss: 1.2158 - val_accuracy: 0.5469\n",
            "Epoch 4/15\n",
            "33/33 [==============================] - 29s 887ms/step - loss: 0.4699 - accuracy: 0.7673 - val_loss: 1.3988 - val_accuracy: 0.6367\n",
            "Epoch 5/15\n",
            "33/33 [==============================] - 30s 899ms/step - loss: 0.4024 - accuracy: 0.8374 - val_loss: 2.2939 - val_accuracy: 0.5938\n",
            "Epoch 6/15\n",
            "33/33 [==============================] - 37s 1s/step - loss: 0.3626 - accuracy: 0.8491 - val_loss: 1.0603 - val_accuracy: 0.7148\n",
            "Epoch 7/15\n",
            "33/33 [==============================] - 33s 963ms/step - loss: 0.2865 - accuracy: 0.8939 - val_loss: 2.0973 - val_accuracy: 0.6562\n",
            "Epoch 8/15\n",
            "33/33 [==============================] - 30s 906ms/step - loss: 0.2836 - accuracy: 0.8978 - val_loss: 0.7723 - val_accuracy: 0.7422\n",
            "Epoch 9/15\n",
            "33/33 [==============================] - 29s 891ms/step - loss: 0.2369 - accuracy: 0.9036 - val_loss: 0.6300 - val_accuracy: 0.8086\n",
            "Epoch 10/15\n",
            "33/33 [==============================] - 29s 892ms/step - loss: 0.1415 - accuracy: 0.9435 - val_loss: 3.9173 - val_accuracy: 0.6094\n",
            "Epoch 11/15\n",
            "33/33 [==============================] - 30s 915ms/step - loss: 0.3006 - accuracy: 0.9260 - val_loss: 0.8254 - val_accuracy: 0.7930\n",
            "Epoch 12/15\n",
            "33/33 [==============================] - 30s 904ms/step - loss: 0.1523 - accuracy: 0.9484 - val_loss: 0.7749 - val_accuracy: 0.8320\n",
            "Epoch 13/15\n",
            "33/33 [==============================] - 30s 898ms/step - loss: 0.1951 - accuracy: 0.9348 - val_loss: 0.8355 - val_accuracy: 0.8320\n",
            "Epoch 14/15\n",
            "33/33 [==============================] - 29s 887ms/step - loss: 0.1215 - accuracy: 0.9679 - val_loss: 0.9712 - val_accuracy: 0.7891\n",
            "Epoch 15/15\n",
            "33/33 [==============================] - 30s 886ms/step - loss: 0.1198 - accuracy: 0.9542 - val_loss: 2.0986 - val_accuracy: 0.7734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jQGslOVc834"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}